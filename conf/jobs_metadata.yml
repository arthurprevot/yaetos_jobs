# Jobs details below. Common job params listed at the bottom of this file.
jobs:
  # examples/ex0_extraction_job.py:
  #   description: "Sample API extraction job, pulling public wikipedia data."
  #   api_inputs: {'path': 'https://raw.githubusercontent.com/wikimedia-research/Discovery-Hiring-Analyst-2016/master/events_log.csv.gz'}
  #   output: {'path':'{base_path}/wiki_example/input/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
  #   frequency: '@once'
  #   spark_boot: False

  # bts/ex1_datadup_job.py:
  #   description: "dup data 100x"
  #   inputs:
  #     pageviews: {'path':"{base_path}/wiki_example/input/{latest}/", 'type':'csv'}
  #   output: {'path':'{base_path}/wiki_example/bts/ex1_datadup_job/{now}/', 'type':'csv'}
  #   dependencies: [examples/ex0_extraction_job.py]
  #   # no_fw_cache: True
  #   ec2_instance_master: 'm5.xlarge'
  #   ec2_instance_slaves: 'm5.xlarge'
  #   emr_core_instances: 2

  # bts/ex1_page_metrics_job.sql:
  #   description: "shows sql job, using spark, for large datasets."
  #   py_job: 'jobs/generic/sql_spark_job.py'
  #   # sql_file: 'jobs/bts/ex1_user_metrics_job.sql'
  #   inputs:
  #     pageviews: {'path':"{base_path}/wiki_example/bts/ex1_datadup_job/{latest}/", 'type':'csv'}
  #   output: {'path':'{base_path}/wiki_example/bts/ex1_page_metrics_job/{now}/', 'type':'csv'}
  #   dependencies: [examples/ex0_extraction_job.py]
  #   repartition: 1
  #   frequency: '@once'
  #   emails: ['some_email@address.com']


  # ----- Climate data (Carbon emissions) Jobs --------
  climate/climate_trace_extraction_job.py:
    description: "Ingest climate trace data. From https://climatetrace.org/. Takes about 10 min to extract."
    # output: {'path':'{base_path}/climate_trace/raw/{now}/dataset.parquet', 'type':'parquet', 'df_type':'pandas'}
    output: {'path':'{base_path}/climate_trace/raw/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False

  bts/climate_trace_asset_dimension_job.py:
    description: "Get info about emitting assets from climate trace data."
    inputs:
      # climate_trace: {'path':"{base_path}/climate_trace/raw/{latest}/dataset.parquet", 'type':'parquet', 'df_type':'pandas'}
      climate_trace: {'path':"{base_path}/climate_trace/raw/{latest}/dataset.csv", 'type':'csv', 'df_type':'pandas'}
    # output: {'path':'{base_path}/climate_trace/asset_dimension/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{base_path}/climate_trace/asset_dimension/{now}/dataset.parquet', 'type':'parquet', 'df_type':'pandas'}
    spark_boot: False
    dependencies: 
      - bts/climate_trace_extraction_job.py

  bts/climate_trace_emission_fact_job.py:
    description: "Get yearly emission data from asset and emission type in climate trace data."
    inputs:
      climate_trace: {'path':"{base_path}/climate_trace/raw/{latest}/dataset.parquet", 'type':'parquet', 'df_type':'pandas'}
    # output: {'path':'{base_path}/climate_trace/emission_facts/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{base_path}/climate_trace/emission_facts/{now}/dataset.parquet', 'type':'parquet', 'df_type':'pandas'}
    spark_boot: False
    dependencies: 
      - bts/climate_trace_extraction_job.py

  bts/emission_fact_expansion_job.py:
    description: "dup data 32x to get to >100M rows"
    inputs:
      pageviews: {'path':"{base_path}/climate_trace/emission_facts/{latest}/", 'type':'parquet'}
    output: {'path':'{base_path}/climate_trace/emission_facts_expanded/{now}/', 'type':'parquet'}
    dependencies: [examples/ex0_extraction_job.py]
    # no_fw_cache: True
    ec2_instance_master: 'm5.xlarge'
    ec2_instance_slaves: 'm5.xlarge'
    emr_core_instances: 2

  climate/climate_trace_assets_master_california_job.py:
    description: "."
    inputs:
      assets_dimension: {'path':'{base_path}/climate_trace/asset_dimension/{latest}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
      emission_facts: {'path':'{base_path}/climate_trace/emission_facts/{latest}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{base_path}/climate_trace/assets_master_california/{now}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False
    dependencies:
      - climate/climate_trace_asset_dimension_job.py
      - climate/climate_trace_emission_fact_job.py

  climate/run_all:
    description: "Run all climate-trace jobs"
    py_job: jobs/generic/dummy_job.py
    spark_boot: False
    dependencies: 
      # - climate/climate_trace_extraction_job.py
      - climate/climate_trace_asset_dimension_job.py
      - climate/climate_trace_emission_fact_job.py

  examples/ex12_run_scala_job:
    description: "Sample spark scala code (compiled into a jar), executed through spark-submit"
    jar_job: 'jobs/examples/ex12_scala_job/target/spark_scala_job_2.13-1.0.jar'
    scala_job: 'jobs/examples/ex12_scala_job/src/spark_scala_job.scala'  # for ref, compilation to be done manually
    sbt: 'jobs/examples/ex12_scala_job/build.sbt'  # for ref, compilation to be done manually
    spark_submit_args: '--verbose'
    spark_app_args: '{base_path}/wordcount_example/input/sample_text.txt'
    load_connectors: none
  
  examples/ex13_register_athena_job:
    description: "Job to demo registering data to Athena. Setup to be run locally, while pushing data to cloud and registering data to athena in AWS."
    py_job: jobs/examples/ex1_frameworked_job.py
    inputs:
      some_events: {'path':"./data/wiki_example/input/{latest}/", 'type':'csv'}
      other_events: {'path':"./data/wiki_example/input/{latest}/", 'type':'csv'}
    output: {'path':'{base_path}/wiki_example/output_ex7_pandas/{now}/', 'type':'csv'}
    base_path: 's3a://mylake-dev/pipelines_data'
    register_to_athena: {'table': 'sandbox.ex1_frameworked'}  # implies sandbox schema created, to be done with "CREATE DATABASE sandbox;" in SQL editor
    enable_db_push: True
    aws_config_file:  conf/aws_config.cfg
    aws_setup: dev
    athena_out: s3://mylake-dev/pipelines_data/athena_data/
    load_connectors: all


  # ----- Dashboards--------
  dashboards/climate_trace.ipynb:
    description: "Dashboard to visualize climate_trace data."
    inputs:
      assets: {'path':'{base_path}/climate_trace/assets_master_california/{latest}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [climate/climate_trace_assets_master_california_job.py]


# ----- Params -------
common_params:
  all_mode_params:
    base_path: '{{root_path}}/pipelines_data'  # don't add '/' at the end
    s3_dags: '{{root_path}}/pipelines_metadata/airflow_dags'  # determines which airflow instance to use.
    s3_logs: '{{root_path}}/pipelines_metadata'
    connection_file:  conf/connections.cfg
    redshift_s3_tmp_dir: s3a://dev-spark/tmp_spark/
    email_cred_section: some_email_cred_section  # Section from "connection_file"
    spark_version: '3.5' # options: '2.4', '3.0', '3.4' or '3.5'
    default_aws_modes: 'dev_EMR'
    default_local_modes: 'dev_local'
    aws_modes: ['dev_EMR','prod_EMR']
  mode_specific_params:
    prod_EMR:
      root_path: s3://mylake-prod  # don't add '/' at the end
      schema: frontroom
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        pro
      jobs_folder:      jobs/
      load_connectors: none
      enable_db_push: True
      save_schemas: False
      manage_git_info: True
    dev_EMR:
      # root_path: s3://mylake-dev  # don't add '/' at the end
      root_path: s3://mylake-dev-testap  # don't add '/' at the end
      schema: sandbox
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        dev_AWSAcad
      jobs_folder:      jobs/
      load_connectors: none
      enable_db_push: False
      save_schemas: False
      manage_git_info: False
    dev_local:
      root_path: '.'  # don't add '/' at the end
      base_path: '{{root_path}}/data'  # don't add '/' at the end
      schema: sandbox
      load_connectors: none
      aws_config_file:  none
      enable_db_push: False
      save_schemas: True
      manage_git_info: False
