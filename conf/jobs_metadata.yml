# Jobs details below. Common job params listed at the bottom of this file.
jobs:
  bts/climate_trace_extraction_job.py:
    description: "Ingest climate trace data. From https://climatetrace.org/. Takes about 10 min to extract."
    output: {'path':'{base_path}/climate_trace/raw/{now}/dataset.parquet', 'type':'parquet', 'df_type':'pandas'}
    spark_boot: False

  bts/climate_trace_asset_dimension_job.py:
    description: "Get info about emitting assets from climate trace data."
    inputs:
      climate_trace: {'path':"{base_path}/climate_trace/raw/{latest}/dataset.parquet", 'type':'parquet', 'df_type':'pandas'}
    output: {'path':'{base_path}/climate_trace/asset_dimension/{now}/dataset.parquet', 'type':'parquet', 'df_type':'pandas'}
    spark_boot: False
    dependencies: 
      - bts/climate_trace_extraction_job.py

  bts/climate_trace_emission_fact_job.py:
    description: "Get yearly emission data from asset and emission type in climate trace data."
    inputs:
      climate_trace: {'path':"{base_path}/climate_trace/raw/{latest}/dataset.parquet", 'type':'parquet', 'df_type':'pandas'}
    output: {'path':'{base_path}/climate_trace/emission_facts/{now}/dataset.parquet', 'type':'parquet', 'df_type':'pandas'}
    spark_boot: False
    dependencies: 
      - bts/climate_trace_extraction_job.py

  bts/emission_fact_expansion_job.py:
    description: "dup data 32x to get to >100M rows"
    inputs:
      emissions: {'path':"{base_path}/climate_trace/emission_facts/{latest}/", 'type':'parquet'}
    output: {'path':'{base_path}/climate_trace/emission_facts_expanded/{now}/', 'type':'parquet'}
    dependencies: 
      - bts/climate_trace_emission_fact_job.py
    ec2_instance_master: 'm5.xlarge'
    ec2_instance_slaves: 'm5.xlarge'
    emr_core_instances: 2

  climate/run_all:
    description: "Run all climate-trace jobs"
    py_job: jobs/generic/dummy_job.py
    ec2_instance_master: 'm5.xlarge'
    ec2_instance_slaves: 'm5.xlarge'
    emr_core_instances: 2
    dependencies: 
      - climate/climate_trace_extraction_job.py
      - climate/climate_trace_asset_dimension_job.py
      - climate/climate_trace_emission_fact_job.py
      - climate/emission_fact_expansion_job.py


# ----- Params -------
common_params:
  all_mode_params:
    base_path: '{root_path}/pipelines_data'  # don't add '/' at the end
    s3_dags: '{root_path}/pipelines_metadata/airflow_dags'  # determines which airflow instance to use.
    s3_logs: '{root_path}/pipelines_metadata'
    connection_file:  conf/connections.cfg
    redshift_s3_tmp_dir: s3a://dev-spark/tmp_spark/
    email_cred_section: some_email_cred_section  # Section from "connection_file"
    spark_version: '3.5' # options: '2.4', '3.0', '3.4' or '3.5'
  mode_specific_params:
    prod_EMR:
      root_path: s3://mylake-prod  # don't add '/' at the end
      schema: frontroom
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        pro
      jobs_folder:      jobs/
      load_connectors: none
      enable_db_push: True
      save_schemas: False
      manage_git_info: True
    dev_EMR:
      root_path: s3://mylake-dev-testap  # update to yours. don't add '/' at the end
      schema: sandbox
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        dev_AWSAcad
      jobs_folder:      jobs/
      load_connectors: none
      enable_db_push: False
      save_schemas: False
      manage_git_info: True
    dev_local:
      root_path: '.'  # don't add '/' at the end
      base_path: '{root_path}/data'  # don't add '/' at the end
      schema: sandbox
      load_connectors: none
      aws_config_file:  none
      enable_db_push: False
      save_schemas: True
      manage_git_info: False
