# Jobs details below. Common job params listed at the bottom of this file.
jobs:
  examples/ex0_extraction_job.py:
    description: "Sample API extraction job, pulling public wikipedia data."
    api_inputs: {'path': 'https://raw.githubusercontent.com/wikimedia-research/Discovery-Hiring-Analyst-2016/master/events_log.csv.gz'}
    output: {'path':'{{base_path}}/wiki_example/input/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    frequency: '@once'
    spark_boot: False

  examples/ex1_sql_job.sql:
    description: "shows sql job, easiest when sql is enough"
    py_job: 'jobs/generic/sql_pandas_job.py'
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv', 'df_type':'pandas'}
      other_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{{base_path}}/wiki_example_sql/output_ex1_sql_pandas/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [examples/ex0_extraction_job.py]
    spark_boot: False
    frequency: '@daily'
    start_date: '{today}T07:00:00+00:00'
    emails: ['some_email@address.com']

  examples/ex7_pandas_job.py:
    description: "job loading and processing data with pandas. No spark involved."
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv', 'df_type':'pandas'}
      other_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv', 'df_type':'pandas', 'read_kwargs':{}}
    output: {'path':'{{base_path}}/wiki_example/output_ex7_pandas/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas', 'save_kwargs':{'sep':'|'}}
    dependencies: [examples/ex0_extraction_job.py]
    frequency: '@once'
    airflow.default_args.retries: 3
    airflow.default_args.retry_delay: 'timedelta(minutes=5)'
    spark_boot: False

  examples/ex1_sql_spark_job:
    description: "shows sql job, using spark, for large datasets."
    py_job: 'jobs/generic/sql_spark_job.py'
    sql_file: 'jobs/examples/ex1_sql_job.sql'
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
      other_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
    output: {'path':'{{base_path}}/wiki_example_sql/output_ex1_sql_spark/{{now}}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]
    repartition: 1
    frequency: '@once'
    emails: ['some_email@address.com']

  examples/ex1_frameworked_job.py:
    description: "shows frameworked pyspark ops, same as ex1_sql_job but gives access to spark ops to expand on sql."
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
      other_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
    output: {'path':'{{base_path}}/wiki_example/output_ex1_frameworked/{{now}}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]
    frequency: '@once'
    emails: ['some_email@address.com']

  job_using_generic_template:
    description: "to show how to reuse existing job code"
    py_job: 'jobs/examples/ex1_frameworked_job.py'
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
      other_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
    output: {'path':'{{base_path}}/wiki_example/output_ex1_frameworked_p2/{{now}}/', 'type':'csv'}
    dependencies: [examples/ex0_extraction_job.py]

  examples/ex1_raw_job.py:
    frequency: 24h
    emails: ['some_email@address.com']

  examples/ex2_frameworked_job.py:
    description: "more complex version of ex1_frameworked_job"
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
      other_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
    output: {'path':'{{base_path}}/wiki_example/output_ex2/{{now}}/', 'type':'csv'}
    frequency: '@once'
    start_date: "{today}T07:00:00+00:00"
    emails: ['some_email@address.com']

  examples/ex3_incremental_job.py:
    description: "focus on incremental loading and dropping"
    inputs:
      processed_events: {'path':"{{base_path}}/wiki_example/output_ex3_dep/{{latest}}/", 'type':'csv', 'inc_field': 'timestamp_obj'}
    output: {'path':'{{base_path}}/wiki_example/output_ex3_inc/incremental_build_v1/', 'type':'csv', 'inc_field': 'other_timestamp'}
    dependencies: [examples/ex3_incremental_prep_job.py]
    frequency: 24h
    emails: ['some_email@address.com']

  examples/ex3_incremental_prep_job.py:
    description: "shows computation of dependency as necessary for ex3_incremental_job"
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
    output: {'path':'{{base_path}}/wiki_example/output_ex3_dep/{{now}}/', 'type':'csv'}
    frequency: 24h
    emails: ['some_email@address.com']

  examples/ex4_dependency1_job.py:
    description: "shows dependency"
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
    output: {'path':'{{base_path}}/wiki_example/output_ex4_dep1/{{now}}/', 'type':'csv'}

  examples/ex4_dependency2_job.py:
    description: "shows dependency without specifying inputs path since it is pulled from upstream."
    inputs:
      some_events: {'path':'{{base_path}}/wiki_example/output_ex4_dep1/{{latest}}/', 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{{base_path}}/wiki_example/output_ex4_dep2/{{now}}/dataset.csv', 'df_type':'pandas', 'type':'csv'}
    dependencies: [examples/ex4_dependency1_job.py]

  examples/ex4_dependency3_job.sql:
    description: "shows dependency with sql"
    py_job: 'jobs/generic/sql_pandas_job.py'
    inputs:
      some_events: {'path':'{{base_path}}/wiki_example/output_ex4_dep2/{{latest}}/', 'type':'csv', 'df_type':'pandas', 'from':'examples/ex4_dependency2_job.py'}  # 'path' not needed when run as dependency
    output: {'path':'{{base_path}}/wiki_example/output_ex4_dep3/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [examples/ex4_dependency2_job.py, examples/ex4_dependency1_job.py]
    spark_boot: False

  examples/ex4_dependency4_job.py:
    description: "shows dependency"
    inputs:
      some_events: {'path':'{{base_path}}/wiki_example/output_ex4_dep3/{{latest}}/', 'type':'csv', 'from':'examples/ex4_dependency3_job.sql'}
    output: {'path':'{{base_path}}/wiki_example/output_ex4_dep4/{{now}}/', 'type':'csv'}
    dependencies: [examples/ex4_dependency3_job.sql]

  examples/ex5_copy_to_oracle_job.py:
    description: "shows frameworked pyspark ops, same as ex1_sql_job but gives access to spark ops to expand on sql."
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
      other_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
    output: {'path':'{{base_path}}/wiki_example/output_ex5_copy_to_oracle/{{now}}/', 'type':'csv'}
    copy_to_oracle: {'creds': 'oracle', 'table': 'sandbox.test_ex5_pyspark_job'}

  examples/ex5_copy_to_redshift_job.py:
    description: "shows frameworked pyspark ops, same as ex1_sql_job but gives access to spark ops to expand on sql."
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
      other_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
    output: {'path':'{{base_path}}/wiki_example/output_ex5_copy_to_oracle/{{now}}/', 'type':'csv'}
    copy_to_redshift: {'creds': 'some_redshift_cred_section', 'table': '{schema}.test_ex5_pyspark_job'}

  examples/ex6_mysql_job.py:
    description: "requires mysql instance running"
    api_inputs: {'api_creds': 'some_mysql_cred_section', 'note':'API Job that relies on creds from conf/connections.cfg'}
    output: {'path':'{{base_path}}/mysql_example/output_ex6_mysql/{{now}}/', 'type':'csv'}

  job_with_no_output:
    description: "shows job with no output (still requiring table as output but not dumped to disk)"
    py_job: 'jobs/examples/ex1_frameworked_job.py'
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
      other_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
    output: {'path':'n/a', 'type':'None'}

  job_with_more_resources:
    description: "To show how to change the machine specs and size of the cluster. See ec2_instance_master and emr_core_instances params below"
    py_job: 'jobs/examples/ex1_frameworked_job.py'
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
      other_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
    output: {'path':'{{base_path}}/wiki_example/output_ex1_frameworked_p3/{{now}}/', 'type':'csv'}
    ec2_instance_master: 'm5.4xlarge'
    ec2_instance_slaves: 'm5.4xlarge'
    emr_core_instances: 3

  examples/ex7_pandas_with_sql_job.py:
    description: "job loading and processing data with pandas. No spark involved."
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv', 'df_type':'pandas'}
      other_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv', 'df_type':'pandas', 'read_kwargs':{}}
    output: {'path':'{{base_path}}/wiki_example/output_ex7_pandas_with_sql/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas', 'save_kwargs':{'sep':'|'}}
    spark_boot: False

  examples/ex7_hybrid_pandas_spark_job.py:
    description: "Job processing in pandas, loading and dropping done with spark."
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
      other_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
    output: {'path':'{{base_path}}/wiki_example/output_ex7_hybrid_pandas_spark/{{now}}/', 'type':'csv'}

  examples/ex8_koalas_job.py:
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
      other_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
    output: {'path':'{{base_path}}/wiki_example/output_ex8_koalas/{{now}}/', 'type':'csv'}

  examples/ex9_redshift_job.py:
    inputs:
      some_events: {'path':"{{base_path}}/wiki_example/input/{{latest}}/", 'type':'csv'}
    output: {'path':'{{base_path}}/wiki_example/output_ex9_redshift/{{now}}/', 'type':'csv'}
    copy_to_redshift: {'creds': 'some_redshift_cred_section', 'table': '{schema}.test_ex9_redshift'}

  examples/ex9_mysql_job.py:
    db_inputs: {'creds': 'some_mysql_cred_section', 'note':'API Job that relies on creds from conf/connections.cfg'}
    output: {'path':'{{base_path}}/wiki_example/output_ex9_mysql/{{now}}/', 'type':'csv'}

  examples/ex9_mysql_framework_load_job:
    py_job: jobs/generic/copy_job.py
    inputs:
      table_to_copy: {'type':'mysql', 'db_table': 'some_schema.some_table', 'creds': 'some_mysql_cred_section', 'note':'creds defined in conf/connections.cfg'}
    output: {'path':'{{base_path}}/db_example/output_ex9_mysql_direct/{{now}}/', 'type':'csv'}
    load_connectors: all
    enable_db_push: True

  examples/ex9_clickhouse_job.py:
    db_inputs: {'creds': 'some_clickhouse_cred_section', 'note':'API Job that relies on creds from conf/connections.cfg'}
    output: {'path':'{{base_path}}/db_example/output_ex9_clickhouse/{{now}}/', 'type':'csv'}

  examples/ex9_clickhouse_framework_load_job:
    py_job: jobs/generic/copy_job.py
    inputs:
      table_to_copy: {'type':'clickhouse', 'db_table': 'some_schema.some_table', 'creds': 'some_clickhouse_cred_section', 'note':'creds defined in conf/connections.cfg'}
    output: {'path':'{{base_path}}/db_example/output_ex9_clickhouse_direct/{{now}}/', 'type':'csv'}
    copy_to_clickhouse: {'creds': 'some_clickhouse_cred_section', 'table': 'public.test_push_arthur'}
    load_connectors: all
    enable_db_push: True

  examples/ex10_excel_load_job:
    py_job: jobs/generic/copy_job.py
    inputs:
      table_to_copy: {'path':"tests/fixtures/data_sample/wiki_example/input_excel/parts.xlsx", 'type':'xlsx', 'df_type':'pandas', 'read_kwargs':{'engine': 'openpyxl', 'sheet_name': 0, 'header': 1}}  # for xls files, use 'type':'xls' and 'engine': 'xlrd'
    output: {'path':'{{base_path}}/load_example/excel/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False

  examples/copy_local_to_cloud_job:
    description: "Copy data from local to cloud. To be executed from local only."
    py_job: jobs/generic/copy_job.py
    inputs:
      table_to_copy: {'path':"tests/fixtures/data_sample/wiki_example/input/", 'type':'csv'}
    output: {'path':'{{base_path}}/load_example/test_files/{{now}}/', 'type':'csv'}
    base_path: 's3a://mylake-dev/pipelines_data'
    aws_config_file:  conf/aws_config.cfg
    aws_setup: dev
    load_connectors: all

  examples/ex11_run_gpu_for_deeplearning.py:
    description: ""
    inputs:
      training_set: {'path':'{{base_path}}/gen_ai/finetuned_albert/raw/{{latest}}/', 'type':'csv', 'df_type':'pandas'}  # file available in repo in ./tests/fixtures/data_sample/gen_ai/text_classification.csv
    output: {'path':'{{base_path}}/load_example/test_files/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    ec2_instance_master: 'g4dn.xlarge'  # access to this GPU instance may requires special request to AWS.
    emr_core_instances: 0

  examples/wordcount_frameworked_job.py:
    description: "shows raw pyspark rdd ops in framework, same as wordcount_raw_job"
    inputs:
      lines: {'path':"{{base_path}}/wordcount_example/input/sample_text.txt", 'type':'txt'}
    output: {'path':'{{base_path}}/wordcount_example/output_frameworked/{{now}}/', 'type':'txt'}
    dependencies: [] # list here if any
    frequency: 24h
    emails: ['some_email@address.com']

  # wordcount_raw_job: #Job exists but doesn't rely on jobs_metadata entries

  # ----- Marketing Jobs --------
  marketing/github_accounts_extraction_job.py:
    description: "Github API extraction job, pulling account data."
    api_inputs: {'creds': 'github'}
    inputs:
      github_accounts_man: {'path':"./data_manual/github/github_accounts.csv", 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{{base_path}}/github/users/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False

  marketing/github_repos_extraction_job.py:
    description: "Github API extraction job, pulling repo data."
    api_inputs: {'creds': 'github'}
    inputs:
      github_accounts: {'path':"{{base_path}}/github/users/{{latest}}/dataset.csv", 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{{base_path}}/github/user_repos/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [marketing/github_accounts_extraction_job.py]
    spark_boot: False

  marketing/github_contributors_extraction_job.py:
    description: "Github API extraction job, pulling contributor data."
    api_inputs: {'creds': 'github'}
    inputs:
      repos: {'path':"{{base_path}}/github/user_repos/{{latest}}/dataset.csv", 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{{base_path}}/github/contributors/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [marketing/github_repos_extraction_job.py]
    spark_boot: False

  marketing/github_committers_extraction_job.py:
    description: "Github API extraction job, pulling committer data, especially email."
    api_inputs: {'creds': 'github'}
    inputs:
      contributors: {'path':"{{base_path}}/github/contributors/{{latest}}/dataset.csv", 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{{base_path}}/github/committers/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [marketing/github_contributors_extraction_job.py]
    spark_boot: False

  marketing/github_repo_list_extraction_job.py:
    description: "Github API extraction job, pulling repo data."
    api_inputs: {'creds': 'github'}
    output: {'path':'{{base_path}}/github/repos/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False

  marketing/github_traffic_extraction_job.py:
    description: "Github API extraction job, pulling repo data."
    inputs:
      my_repos: {'path':"./data_manual/github/my_github_repos.csv", 'type':'csv', 'df_type':'pandas'}
    api_inputs: {'creds': 'github'}
    output: {'path':'{{base_path}}/github/traffic/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False

  marketing/people_merge_job.py:
    description: "Merge people from linkedin, github..."
    inputs:
      perso: {'path':"./data_manual/people/people.csv", 'type':'csv'}
      linkedin: {'path':"./data_manual/linkedin/export/Connections_mod.csv", 'type':'csv'}
    output: {'path':'{{base_path}}/people/{{now}}/', 'type':'csv'}


  # ----- Lead generation --------
  marketing/apollo_extraction_job.py:
    description: "Extract apollo data from list of companies provided."
    api_inputs: {'creds': 'apollo'}  # Creds to be added to conf/connections.cfg
    inputs:      
      companies: {'path':"./tests/fixtures/data_sample/companies/companies.csv", 'type':'csv', 'df_type':'pandas', 'read_kwargs':{'sep': ';'}}  # Example file to be replaced
    output: {'path':'{{base_path}}/apollo_p1_extraction/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas', 'save_kwargs':{'sep':';'}}
    spark_boot: False

  marketing/apollo_merge_job.py:
    description: "Merge company data with apollo extraction data."
    inputs:
      companies: {'path':"./tests/fixtures/data_sample/companies/companies.csv", 'type':'csv', 'df_type':'pandas', 'read_kwargs':{'sep': ';'}}  # Example file to be replaced
      apollo: {'path':"{{base_path}}/apollo_p1_extraction/{{latest}}/dataset.csv", 'type':'csv', 'df_type':'pandas', 'read_kwargs':{'sep': ';'}}
    output: {'path':'{{base_path}}/apollo_p2_merged/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas', 'save_kwargs':{'sep':';'}}
    spark_boot: False
    dependencies: [marketing/apollo_extraction_job.py]

  # ----- AI & Gen AI --------
  marketing/chatgpt_extraction_job.py:
    description: "Extract chatgpt data from list of companies provided."
    api_inputs: {'creds': 'chatgpt'}
    inputs:
      companies: {'path':"./tests/fixtures/data_sample/companies/companies.csv", 'type':'csv', 'df_type':'pandas', 'read_kwargs':{'sep': ';'}}  # Example file to be replaced
    output: {'path':'{{base_path}}/chatgpt_extraction/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas', 'save_kwargs':{'sep':';'}}
    spark_boot: False

  gen_ai/albert_finetune_copy_data_job:
    description: "Copy data from local to cloud."
    py_job: jobs/generic/copy_job.py
    inputs:
      table_to_copy: {'path':"./tests/fixtures/data_sample/gen_ai/text_classification.csv", 'type':'csv'}  # Example file to be replaced
    output: {'path':'{{base_path}}/gen_ai/finetuned_albert/raw/{{now}}/', 'type':'csv'}
    base_path: 's3a://mylake-dev/pipelines_data'
    aws_config_file:  conf/aws_config.cfg
    aws_setup: dev
    load_connectors: all

  gen_ai/albert_finetune_job.py:
    description: "Finetune an existing language model (ALBERT) to perform classification tasks, using tensorflow and huggingface."
    inputs:
      training_set: {'path':'{{base_path}}/gen_ai/finetuned_albert/raw/{{latest}}/', 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{{base_path}}/gen_ai/finetuned_albert/tests/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    output_model: {'path':'{{base_path}}/gen_ai/finetuned_albert/model/{{now}}/'}
    spark_boot: False
    ec2_instance_master: 'g4dn.xlarge'
    emr_core_instances: 0


  gen_ai/albert_finetuned_inferences_job.py:
    description: "Run classification job. Depends on model and code from job albert_finetune_job.py."
    input_model: {'path':'{{base_path}}/gen_ai/finetuned_albert/model/{{latest}}/'}
    inputs:
      text_to_classify: {'path':"./tests/fixtures/data_sample/gen_ai/text_classification.csv", 'type':'csv', 'df_type':'pandas'}  # Example file to be replaced. This one has labels too but it doesn't have to.
    output: {'path':'{{base_path}}/gen_ai/finetuned_albert/classified/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [gen_ai/albert_finetune_job.py]
    spark_boot: False

  gen_ai/privateGPT_document_feeder_job.py:
    description: "Feed documents to the privateGPT vector database."
    host_privategpt: 'http://localhost:8001'  # To be updated according to your host.
    inputs:
      listing: {'path':"./tests/fixtures/data_sample/gen_ai/listing.csv", 'type':'csv', 'df_type':'pandas'}  # Example file to be replaced
    output: {'path':'{{base_path}}/gen_ai/privateGPT_feeder/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False

  # ----- Image processing --------
  image_processing/image_processing_job.py:
    description: "Perform contour detection in images. Will pickup and drop the images in the same folder as the listing file."
    inputs:
      listing: {'path':"./tests/fixtures/data_sample/images/listing.csv", 'type':'csv'}  # Example file to be replaced
    output: {'path':'{{base_path}}/images_processing/2_processed/listing/{{now}}/', 'type':'csv'}

  # ----- Climate data (Carbon emissions) Jobs --------
  climate/climate_trace_extraction_job.py:
    description: "Ingest climate trace data. From https://climatetrace.org/."
    output: {'path':'{{base_path}}/climate_trace/raw/{{now}}/dataset.parquet', 'type':'parquet', 'df_type':'pandas'}
    spark_boot: False

  climate/climate_trace_asset_dimension_job.py:
    description: "Get info about emitting assets from climate trace data."
    inputs:
      climate_trace: {'path':"{{base_path}}/climate_trace/raw/{{latest}}/dataset.parquet", 'type':'parquet', 'df_type':'pandas'}
    output: {'path':'{{base_path}}/climate_trace/asset_dimension/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False
    dependencies: [climate/climate_trace_extraction_job.py]

  climate/climate_trace_emission_fact_job.py:
    description: "Get yearly emission data from asset and emission type in climate trace data."
    inputs:
      climate_trace: {'path':"{{base_path}}/climate_trace/raw/{{latest}}/dataset.parquet", 'type':'parquet', 'df_type':'pandas'}
    output: {'path':'{{base_path}}/climate_trace/emission_facts/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False
    dependencies: [climate/climate_trace_extraction_job.py]

  climate/climate_trace_assets_master_california_job.py:
    description: "."
    inputs:
      assets_dimension: {'path':'{{base_path}}/climate_trace/asset_dimension/{{latest}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
      emission_facts: {'path':'{{base_path}}/climate_trace/emission_facts/{{latest}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    output: {'path':'{{base_path}}/climate_trace/assets_master_california/{{now}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    spark_boot: False
    dependencies:
      - climate/climate_trace_asset_dimension_job.py
      - climate/climate_trace_emission_fact_job.py

  climate/run_all:
    description: "Run all climate-trace jobs"
    py_job: jobs/generic/dummy_job.py
    spark_boot: False
    dependencies: 
      - climate/climate_trace_extraction_job.py
      - climate/climate_trace_asset_dimension_job.py
      - climate/climate_trace_emission_fact_job.py

  examples/ex12_run_scala_job:
    description: "Sample spark scala code (compiled into a jar), executed through spark-submit"
    jar_job: 'jobs/examples/ex12_scala_job/target/spark_scala_job_2.13-1.0.jar'
    scala_job: 'jobs/examples/ex12_scala_job/src/spark_scala_job.scala'  # for ref, compilation to be done manually
    sbt: 'jobs/examples/ex12_scala_job/build.sbt'  # for ref, compilation to be done manually
    spark_submit_args: '--verbose'
    spark_app_args: '{{base_path}}/wordcount_example/input/sample_text.txt'
    load_connectors: none
  
  examples/ex13_register_athena_job:
    description: "Job to demo registering data to Athena. Setup to be run locally, while pushing data to cloud and registering data to athena in AWS."
    py_job: jobs/examples/ex1_frameworked_job.py
    inputs:
      some_events: {'path':"./data/wiki_example/input/{{latest}}/", 'type':'csv'}
      other_events: {'path':"./data/wiki_example/input/{{latest}}/", 'type':'csv'}
    output: {'path':'{{base_path}}/wiki_example/output_ex7_pandas/{{now}}/', 'type':'csv'}
    base_path: 's3a://mylake-dev/pipelines_data'
    register_to_athena: {'table': 'sandbox.ex1_frameworked'}  # implies sandbox schema created, to be done with "CREATE DATABASE sandbox;" in SQL editor
    enable_db_push: True
    aws_config_file:  conf/aws_config.cfg
    aws_setup: dev
    athena_out: s3://mylake-dev/pipelines_data/athena_data/
    load_connectors: all


  # ----- Dashboards--------
  dashboards/climate_trace.ipynb:
    description: "Dashboard to visualize climate_trace data."
    inputs:
      assets: {'path':'{{base_path}}/climate_trace/assets_master_california/{{latest}}/dataset.csv', 'type':'csv', 'df_type':'pandas'}
    dependencies: [climate/climate_trace_assets_master_california_job.py]


# ----- Params -------
common_params:
  all_mode_params:
    base_path: '{{root_path}}/pipelines_data'  # don't add '/' at the end
    s3_dags: '{{root_path}}/pipelines_metadata/airflow_dags'  # determines which airflow instance to use.
    s3_logs: '{{root_path}}/pipelines_metadata'
    connection_file:  conf/connections.cfg
    redshift_s3_tmp_dir: s3a://dev-spark/tmp_spark/
    email_cred_section: some_email_cred_section  # Section from "connection_file"
    spark_version: '3.5' # options: '2.4', '3.0', '3.4' or '3.5'
    default_aws_modes: 'dev_EMR'
    default_local_modes: 'dev_local'
    aws_modes: ['dev_EMR','prod_EMR']
  mode_specific_params:
    prod_EMR:
      root_path: s3://mylake-prod  # don't add '/' at the end
      schema: frontroom
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        pro
      jobs_folder:      jobs/
      load_connectors: none
      enable_db_push: True
      save_schemas: False
      manage_git_info: True
    dev_EMR:
      root_path: s3://mylake-dev-testap2  # don't add '/' at the end
      schema: sandbox
      emr_core_instances: 0
      aws_config_file:  conf/aws_config.cfg
      aws_setup:        dev_BTS
      jobs_folder:      jobs/
      load_connectors: none
      enable_db_push: False
      save_schemas: False
      manage_git_info: False
    dev_local:
      root_path: '.'  # don't add '/' at the end
      base_path: '{{root_path}}/data'  # don't add '/' at the end
      schema: sandbox
      load_connectors: none
      aws_config_file:  none
      enable_db_push: False
      save_schemas: True
      manage_git_info: False
